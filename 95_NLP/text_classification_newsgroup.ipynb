{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8622ff085c15f6cc9f9bc345308196ca585eaf1c49b171850405ec6bc4303143"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 라벨링이 되어 있음\n",
    "# 라벨링의 분포는 고른 편\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)\n",
    "print(news_data.keys())\n",
    "print(pd.Series(news_data.target).value_counts().sort_index())\n",
    "print(news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train & test\n",
    "\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 등장한 단어를 기반으로 벡터화한다\n",
    "# count 가 단순히 카운트로 수행한다면 tfidf 는 공통으로 많이 등장하는 단어는 제거한다\n",
    "# train data와 test data를 벡터로 변환하는 테이블이 동일해야 함에 주의한다\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6736590546999469\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "score = accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화\n",
    "# tfidf의 파라미터는 스톱워드, ngram_range, max_df 등이 있다\n",
    "# logistic 의 파라미터는 feature selection을 위한 C가 있다\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "score = accuracy_score(y_test, pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C':[0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
    "best_C = grid_cv_lr.best_params_\n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "score = accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 pileline로 벡터화부터 분류까지 한번에 수행하는 파이프라인을 구축할 수 있다\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr_clf', LogisticRegression(random_state=156))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "score = accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'tfidf_vect__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "          'tfidf_vect__max_df':[0.7, 0.8, 0.9],\n",
    "          'lr_clf__C':[1,5,10]}\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_pipe.fit(X_train, y_train)\n",
    "best_param = grid_cv_pipe.best_params_\n",
    "score = accuracy_score(y_test, pred)"
   ]
  }
 ]
}